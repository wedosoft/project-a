# Task ID: 5
# Title: Freshdesk API 연동 모듈 개발
# Status: completed
# Dependencies: 1
# Priority: high
# Description: Freshdesk API를 통한 티켓 및 솔루션 데이터 수집 후 OpenAI 임베딩 모델을 사용하여 벡터로 변환하고 Qdrant 벡터 DB에 저장하는 파이프라인 구축
# Details:
Freshdesk API 연동 모듈 개발 및 수집된 데이터의 임베딩 파이프라인 구축:

1. Freshdesk API 연동:
- httpx 패키지를 활용한 비동기 HTTP 클라이언트 구현
- Freshdesk API 인증 및 기본 설정
- 티켓 데이터 수집 함수 구현(페이지네이션 처리, 대용량 데이터 고려)
- 솔루션 문서 수집 함수 구현
- 첨부파일 다운로드 및 메타데이터 추출 기능
- Rate limiting 및 API 할당량 관리
- 에러 처리 및 재시도 로직

2. 임베딩 및 Qdrant 저장 파이프라인:
- `core/embedding_pipeline.py` 또는 `data/embedding_service.py` 등에 핵심 로직 구현
- OpenAI API 클라이언트는 `OPENAI_API_KEY` 환경 변수를 사용하여 초기화
- 임베딩 모델은 `EMBEDDING_MODEL` 환경 변수(`text-embedding-3-small` 등)를 참조하여 동적으로 로드
- 텍스트 데이터를 임베딩 모델의 최대 토큰 길이에 맞춰 청크로 분할하는 로직 구현 (예: `text-embedding-3-small`의 경우 8191 토큰)
- 임베딩 생성 시 배치 처리를 활용하여 API 호출 효율성 향상
- 생성된 벡터와 원본 텍스트, Freshdesk 메타데이터(`ticket_id`, `solution_id`, `company_id`, `created_at`, `updated_at` 등)를 Qdrant에 저장
- Qdrant 저장 시 `upsert_points` API를 사용하며, `company_id`를 포함한 페이로드 구성
- 임베딩 생성 및 Qdrant 저장 과정에서의 오류 처리 및 로깅
- 대용량 데이터 처리 시 메모리 사용량 및 처리 시간 최적화
- 모든 코드에 상세한 한글 주석 작성

# Test Strategy:
1. Freshdesk API 연동 테스트:
- Freshdesk API 연결 테스트
- 티켓 및 솔루션 데이터 수집 정확성 테스트
- 페이지네이션 처리 테스트
- 에러 처리 및 재시도 로직 테스트
- Rate limiting 준수 테스트
- 메모리 사용량 모니터링

2. 임베딩 및 Qdrant 저장 테스트:
- OpenAI API 연결 및 임베딩 모델 로딩 기능 테스트
- 텍스트 청킹 로직의 정확성 검증 (다양한 길이의 텍스트 입력)
- 임베딩 생성 결과의 유효성 확인 (벡터 차원, 값 범위 등)
- Qdrant 저장 로직 테스트 (데이터 정확성, 메타데이터 포함 여부)
- 배치 처리 기능 및 성능 테스트
- 다양한 오류 시나리오(API 키 오류, 네트워크 오류, Qdrant 연결 실패 등)에 대한 에러 처리 및 로깅 검증
- `company_id` 기반 데이터 분리 저장 확인

# Subtasks:
## 5.1. Freshdesk API 클라이언트 구현 [pending]
### Dependencies: None
### Description: httpx 패키지를 활용한 비동기 HTTP 클라이언트 및 Freshdesk API 인증 구현
### Details:


## 5.2. 티켓 및 솔루션 데이터 수집 함수 개발 [pending]
### Dependencies: None
### Description: 페이지네이션 처리 및 대용량 데이터를 고려한 티켓 및 솔루션 문서 수집 함수 구현
### Details:


## 5.3. 첨부파일 처리 및 메타데이터 추출 기능 개발 [pending]
### Dependencies: None
### Description: Freshdesk 티켓 및 솔루션의 첨부파일 다운로드 및 메타데이터 추출 기능 구현
### Details:


## 5.4. OpenAI API 클라이언트 초기화 및 임베딩 모델 로더 구현 [pending]
### Dependencies: None
### Description: `OPENAI_API_KEY` 환경 변수를 사용한 API 클라이언트 초기화 및 `EMBEDDING_MODEL` 환경 변수를 참조하여 임베딩 모델 동적 로드 기능 구현
### Details:


## 5.5. 텍스트 데이터 청킹 유틸리티 개발 [pending]
### Dependencies: None
### Description: 임베딩 모델의 최대 토큰 길이에 맞춰 텍스트 데이터를 청크로 분할하는 유틸리티 개발 (모델별 최대 토큰 길이 고려)
### Details:


## 5.6. OpenAI 임베딩 생성 서비스 구현 [pending]
### Dependencies: None
### Description: 배치 처리 기능을 포함한 OpenAI 임베딩 생성 서비스 구현 (API 호출 효율성 최적화)
### Details:


## 5.7. Qdrant 저장용 데이터 구조 정의 및 변환 로직 개발 [pending]
### Dependencies: None
### Description: 벡터, 원본 텍스트, Freshdesk 메타데이터(`ticket_id`, `solution_id`, `company_id` 등)를 포함한 Qdrant 저장용 데이터 구조 정의 및 변환 로직 개발
### Details:


## 5.8. Qdrant 포인트 업서트 로직 구현 [pending]
### Dependencies: None
### Description: `upsert_points` API를 사용하여 임베딩 벡터와 메타데이터를 Qdrant에 저장하는 로직 구현 (`core/vectordb.py` 또는 `data/qdrant_handler.py`의 함수 활용)
### Details:


## 5.9. 임베딩 및 저장 파이프라인 전체 흐름 제어 로직 개발 [pending]
### Dependencies: None
### Description: Freshdesk 데이터 수집부터 임베딩 생성, Qdrant 저장까지의 전체 파이프라인 흐름을 제어하는 로직 개발
### Details:


## 5.10. 에러 처리 및 구조화된 로깅 강화 [pending]
### Dependencies: None
### Description: 임베딩 생성 및 Qdrant 저장 과정에서의 오류(OpenAI API 오류, Qdrant 연결 오류, 데이터 형식 오류 등)를 상세히 처리하고 구조화된 로깅 구현
### Details:


## 5.11. 단위 테스트 및 통합 테스트 작성 [pending]
### Dependencies: None
### Description: 임베딩 및 Qdrant 저장 관련 단위 테스트 및 통합 테스트 작성 (`tests/core/` 또는 `tests/data/`)
### Details:


## 12. Text Chunking and Preprocessing Strategies [pending]
### Dependencies: None
### Description: Develop and implement text chunking and preprocessing strategies optimized for different content types (tickets, knowledge base articles, documentation).
### Details:
Design and implement preprocessing pipelines that include: tokenization, sentence splitting, chunking strategies (fixed-size, semantic, hybrid approaches), metadata preservation during chunking, content-specific preprocessing rules, handling of special characters and formatting, and normalization techniques. Create detailed data flow diagrams showing the preprocessing steps for each content type. Document the rationale behind each chunking strategy and its appropriateness for different content types.

## 13. Ticket Processing Workflow Implementation [pending]
### Dependencies: 5.12
### Description: Design and implement the complete workflow for processing support tickets from ingestion to embedding generation and storage.
### Details:
Create a comprehensive ticket processing pipeline that includes: ticket data extraction, metadata parsing, content cleaning, applying the appropriate chunking strategy, embedding generation for each chunk, metadata association with embeddings, vector database storage with proper indexing, and logging/monitoring of the process. Develop data flow diagrams illustrating the complete ticket processing workflow from source to storage. Include error handling and retry mechanisms for each processing stage.

## 14. Knowledge Base Article Processing Workflow [pending]
### Dependencies: 5.12
### Description: Design and implement the complete workflow for processing knowledge base articles from ingestion to embedding generation and storage.
### Details:
Create a comprehensive knowledge base article processing pipeline that includes: article data extraction, metadata parsing, content cleaning, applying the appropriate chunking strategy, handling of structured sections and hierarchies, embedding generation for each chunk, metadata association with embeddings, vector database storage with proper indexing, and logging/monitoring of the process. Develop data flow diagrams illustrating the complete knowledge base article processing workflow. Include special handling for article relationships, versioning, and hierarchical structure preservation.

## 15. Incremental Update and Batch Processing Optimization [pending]
### Dependencies: 5.13, 5.14
### Description: Design and implement systems for efficient incremental updates and optimized batch processing of content.
### Details:
Develop mechanisms for: detecting and processing only changed/new content, efficient batch processing for large content volumes, scheduling strategies for regular updates, optimizing resource usage during processing, handling of deleted content, maintaining consistency between source data and embeddings, and performance monitoring. Create detailed data flow diagrams for both incremental update processes and batch processing workflows. Implement benchmarking tools to measure and optimize processing throughput and resource utilization.

