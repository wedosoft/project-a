# Task ID: 12
# Title: Implement Caching and Performance Optimization
# Status: pending
# Dependencies: 8, 9, 10, 11
# Priority: medium
# Description: Create a caching layer and performance optimizations to meet the response time requirements of 2 seconds or less for API endpoints.
# Details:
1. Implement Redis-based caching for:
   - API responses
   - LLM responses
   - Vector search results
   - Embeddings
2. Add cache invalidation strategies
3. Implement background processing for non-critical operations
4. Create connection pooling for database and API clients
5. Add request batching for external API calls
6. Implement response compression
7. Create performance monitoring and metrics

Example code:
```python
from fastapi import FastAPI, Request, Response
from fastapi.middleware.gzip import GZipMiddleware
import redis.asyncio as redis
import json
import time
from functools import wraps

app = FastAPI()
app.add_middleware(GZipMiddleware, minimum_size=1000)

class CacheService:
    def __init__(self, redis_url: str, default_ttl: int = 3600):
        self.redis = redis.from_url(redis_url)
        self.default_ttl = default_ttl
        
    async def get(self, key: str):
        value = await self.redis.get(key)
        if value:
            return json.loads(value)
        return None
        
    async def set(self, key: str, value: any, ttl: int = None):
        if ttl is None:
            ttl = self.default_ttl
        await self.redis.setex(key, ttl, json.dumps(value))
        
    async def delete(self, key: str):
        await self.redis.delete(key)
        
    async def invalidate_pattern(self, pattern: str):
        keys = await self.redis.keys(pattern)
        if keys:
            await self.redis.delete(*keys)

def cached_endpoint(ttl: int = 3600):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # Get request from kwargs
            request = next((arg for arg in args if isinstance(arg, Request)), 
                          kwargs.get('req') or kwargs.get('request'))
                          
            if not request:
                return await func(*args, **kwargs)
                
            # Create cache key from request details
            cache_service = request.app.state.cache_service
            path = request.url.path
            query_string = str(request.query_params)
            body = await request.body()
            body_str = body.decode() if body else ""
            
            # Include company_id in cache key for multi-tenant isolation
            company_id = None
            if body_str:
                try:
                    body_json = json.loads(body_str)
                    company_id = body_json.get("company_id")
                except:
                    pass
                    
            cache_key = f"api:{path}:{query_string}:{body_str}"
            if company_id:
                cache_key = f"company:{company_id}:{cache_key}"
                
            # Try to get from cache
            cached_response = await cache_service.get(cache_key)
            if cached_response:
                return Response(
                    content=cached_response["content"],
                    status_code=cached_response["status_code"],
                    headers=cached_response["headers"],
                    media_type=cached_response["media_type"]
                )
                
            # Execute the endpoint function
            response = await func(*args, **kwargs)
            
            # Cache the response
            if hasattr(response, "body"):
                await cache_service.set(
                    cache_key,
                    {
                        "content": response.body.decode(),
                        "status_code": response.status_code,
                        "headers": dict(response.headers),
                        "media_type": response.media_type
                    },
                    ttl
                )
                
            return response
        return wrapper
    return decorator

# Example usage on an endpoint
@app.get("/api/v1/similar_tickets")
@cached_endpoint(ttl=300)  # Cache for 5 minutes
async def get_similar_tickets(request: Request):
    # Endpoint implementation
    pass

# Background task processing
from fastapi.background import BackgroundTasks

@app.post("/api/v1/process_data")
async def process_data(request: Request, background_tasks: BackgroundTasks):
    # Extract data from request
    data = await request.json()
    
    # Add to background tasks
    background_tasks.add_task(process_data_in_background, data)
    
    return {"status": "processing"}
    
async def process_data_in_background(data: dict):
    # Long-running process
    pass
```

# Test Strategy:
1. Measure API response times with and without caching
2. Test cache invalidation strategies
3. Verify multi-tenant isolation in cache
4. Test background processing for long-running tasks
5. Measure memory usage under load
6. Test connection pooling efficiency
7. Verify response compression
8. Load test with simulated concurrent users

# Subtasks:
## 1. Implement Redis-Based Caching Infrastructure [pending]
### Dependencies: None
### Description: Set up and configure a Redis-based distributed caching layer to store frequently accessed data, ensuring high availability and scalability. Define cache namespaces, key structures, and data serialization formats. Apply best practices such as separating cache and session storage, enabling compression, and configuring L2 cache if needed.
### Details:
Follow Redis deployment best practices, including using separate Redis instances for cache and session data, enabling data compression (e.g., gzip), and pre-loading keys for performance. Configure replication and clustering for high availability. Document cache key naming conventions and data expiration policies.

## 2. Design and Implement Cache Invalidation Strategies [pending]
### Dependencies: 12.1
### Description: Develop robust cache invalidation mechanisms to ensure data consistency between the cache and the underlying data store. Choose appropriate invalidation policies (e.g., time-based TTL, event-driven, or manual invalidation) for different data types and usage patterns.
### Details:
Define detailed caching policies, including TTL values for various data types, and implement event-driven invalidation (e.g., on data update or delete). Consider using cache versioning or tagging for bulk invalidation. Document scenarios for cache refresh and fallback strategies.

## 3. Integrate Background Processing for Non-Critical Operations [pending]
### Dependencies: 12.1, 12.2
### Description: Offload non-critical and long-running operations (such as cache warming, batch updates, or analytics) to background processing systems to reduce latency for end-user requests.
### Details:
Implement background jobs for cache pre-warming, periodic cache refresh, and bulk data processing. Use job queues or task schedulers to manage background tasks. Monitor job execution and handle failures gracefully to maintain cache integrity.

## 4. Optimize Connection Pooling and Request Batching [pending]
### Dependencies: 12.1, 12.2, 12.3
### Description: Enhance Redis client performance by configuring connection pooling and batching multiple requests to minimize network overhead and improve throughput.
### Details:
Configure Redis client libraries to use efficient connection pools, set optimal pool sizes, and enable pipelining or batching of requests where applicable. Benchmark performance before and after optimization using metrics such as cache hit rate, latency, and throughput. Document benchmarking methodology and results.

