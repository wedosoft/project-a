# ğŸš€ ë³µì¡í•œ ì¡°ê±´ ê²€ìƒ‰ì„ ìœ„í•œ ìµœì  VectorDB ì €ì¥ êµ¬ì¡° ì„¤ê³„ ì›Œí¬í”Œë¡œìš°

## ğŸ“‹ Executive Summary

ì‚¬ìš©ìê°€ ì±„íŒ…ì„ í†µí•´ ë³µì¡í•œ ì¡°ê±´ì˜ ë¬¸ì„œë¥¼ ì°¾ì„ ë•Œ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” Qdrant ê¸°ë°˜ ìµœì  ì €ì¥ êµ¬ì¡°ë¥¼ ì„¤ê³„í•©ë‹ˆë‹¤. ìì—°ì–´ ì¿¼ë¦¬ë¥¼ êµ¬ì¡°í™”ëœ í•„í„°ë¡œ ë³€í™˜í•˜ê³ , ë‹¤ì°¨ì› ê²€ìƒ‰ì„ ì§€ì›í•˜ëŠ” ê³ ì„±ëŠ¥ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

## ğŸ¯ ìš”êµ¬ì‚¬í•­ ë¶„ì„

### ë³µì¡í•œ ì¡°ê±´ì˜ ì •ì˜
1. **ë‹¤ì¤‘ í•„í„°ë§ ì¡°í•©**
   - ì‹œê°„ ë²”ìœ„: "ì§€ë‚œ ì£¼ì— ìƒì„±ëœ í‹°ì¼“"
   - ì¹´í…Œê³ ë¦¬/íƒœê·¸: "ê²°ì œ ê´€ë ¨ì´ë©´ì„œ í™˜ë¶ˆ íƒœê·¸ê°€ ìˆëŠ”"
   - ìƒíƒœ/ìš°ì„ ìˆœìœ„: "ê¸´ê¸‰í•˜ê³  ë¯¸í•´ê²°ëœ"
   - ì‚¬ìš©ì/ë‹´ë‹¹ì: "ê¹€ì² ìˆ˜ê°€ ë‹´ë‹¹í•˜ëŠ”"

2. **ì˜ë¯¸ ê¸°ë°˜ + ë©”íƒ€ë°ì´í„° í˜¼í•©**
   - "ê²°ì œ ì˜¤ë¥˜ì™€ ìœ ì‚¬í•˜ë©´ì„œ ìµœê·¼ ì¼ì£¼ì¼ ë‚´ í•´ê²°ëœ í‹°ì¼“"
   - "ê³ ê° ë§Œì¡±ë„ê°€ ë‚®ì€ í™˜ë¶ˆ ê´€ë ¨ ë¬¸ì„œ"

3. **ê³„ì¸µì  ì¡°ê±´**
   - "VIP ê³ ê°ì˜ ê¸´ê¸‰ í‹°ì¼“ ì¤‘ 24ì‹œê°„ ì´ìƒ ë¯¸ì‘ë‹µ"

## ğŸ—ï¸ ìµœì í™”ëœ VectorDB ìŠ¤í‚¤ë§ˆ ì„¤ê³„

### 1. ê³„ì¸µì  Payload êµ¬ì¡°
```python
{
    "id": "ticket_123",
    "tenant_id": "company_456",
    
    # í•µì‹¬ ë©”íƒ€ë°ì´í„° (ì¸ë±ì‹±ë¨)
    "core": {
        "doc_type": "ticket",  # ticket/article/kb
        "platform": "freshdesk",
        "created_at": 1704067200,  # Unix timestamp
        "updated_at": 1704153600,
        "status": "open",  # open/pending/resolved/closed
        "priority": 3,  # 1:low, 2:medium, 3:high, 4:urgent
    },
    
    # ë¶„ë¥˜ ì •ë³´ (ì¸ë±ì‹±ë¨)
    "classification": {
        "category": "billing",
        "subcategory": "refund",
        "tags": ["payment", "refund", "urgent"],
        "topics": ["payment_error", "refund_request"]
    },
    
    # ê´€ê³„ ì •ë³´
    "relations": {
        "customer_id": "cust_789",
        "customer_tier": "vip",  # vip/premium/standard
        "agent_id": "agent_012",
        "team": "support_team_a"
    },
    
    # ë©”íŠ¸ë¦­ (ë²”ìœ„ ê²€ìƒ‰ìš©)
    "metrics": {
        "response_time_hours": 2.5,
        "resolution_time_hours": 48,
        "interaction_count": 5,
        "satisfaction_score": 4.2,
        "sentiment_score": 0.8  # -1 to 1
    },
    
    # ì½˜í…ì¸  ì •ë³´
    "content": {
        "title": "ê²°ì œ ì˜¤ë¥˜ ë°œìƒ",
        "language": "ko",
        "word_count": 250,
        "has_attachments": true,
        "attachment_types": ["pdf", "image"]
    },
    
    # ê²€ìƒ‰ ì¦ê°• ì •ë³´
    "search_hints": {
        "keywords": ["ê²°ì œ", "ì˜¤ë¥˜", "í™˜ë¶ˆ"],
        "entities": ["Visa", "2024-01-01"],
        "intent": "refund_request"
    }
}
```

### 2. ë‹¤ì¤‘ ë²¡í„° êµ¬ì¡° (Named Vectors)
```python
{
    "vectors": {
        "content": [...],  # ë³¸ë¬¸ ì„ë² ë”© (dense)
        "title": [...],    # ì œëª© ì„ë² ë”© (dense)
        "keywords": {...}  # í‚¤ì›Œë“œ ì„ë² ë”© (sparse)
    }
}
```

### 3. ì¸ë±ì‹± ì „ëµ
```python
# Qdrant ì»¬ë ‰ì…˜ ìƒì„± ì‹œ payload ì¸ë±ì‹±
indexes = [
    {"field": "core.created_at", "type": "integer"},
    {"field": "core.status", "type": "keyword"},
    {"field": "core.priority", "type": "integer"},
    {"field": "classification.category", "type": "keyword"},
    {"field": "classification.tags", "type": "keyword[]"},
    {"field": "relations.customer_tier", "type": "keyword"},
    {"field": "metrics.satisfaction_score", "type": "float"},
    {"field": "metrics.response_time_hours", "type": "float"}
]
```

## ğŸ“Š êµ¬í˜„ ì›Œí¬í”Œë¡œìš°

### Phase 1: ê¸°ë°˜ êµ¬ì¡° êµ¬ì¶• (Week 1)

#### Task 1.1: ìŠ¤í‚¤ë§ˆ ì •ì˜ ë° ë§ˆì´ê·¸ë ˆì´ì…˜
**ë‹´ë‹¹**: Backend Developer  
**ì˜ˆìƒ ì‹œê°„**: 16ì‹œê°„  
**MCP Context**: Qdrant collection ì„¤ì •, payload êµ¬ì¡° ì„¤ê³„

```python
# backend/core/database/schemas/vector_schema.py
from pydantic import BaseModel
from typing import List, Optional, Dict
from datetime import datetime

class CoreMetadata(BaseModel):
    doc_type: str
    platform: str
    created_at: datetime
    updated_at: datetime
    status: str
    priority: int

class Classification(BaseModel):
    category: str
    subcategory: Optional[str]
    tags: List[str]
    topics: List[str]

class VectorDocument(BaseModel):
    id: str
    tenant_id: str
    core: CoreMetadata
    classification: Classification
    relations: Dict[str, Any]
    metrics: Dict[str, float]
    content: Dict[str, Any]
    search_hints: Dict[str, List[str]]
```

#### Task 1.2: Qdrant ì»¬ë ‰ì…˜ ìµœì í™”
**ë‹´ë‹¹**: Backend Developer  
**ì˜ˆìƒ ì‹œê°„**: 12ì‹œê°„

```python
# backend/core/database/qdrant_optimizer.py
from qdrant_client import QdrantClient
from qdrant_client.models import (
    VectorParams, Distance, PayloadIndexParams,
    PayloadSchemaType, TextIndexParams
)

class QdrantOptimizer:
    def create_optimized_collection(self):
        self.client.create_collection(
            collection_name="documents_v2",
            vectors_config={
                "content": VectorParams(size=3072, distance=Distance.COSINE),
                "title": VectorParams(size=3072, distance=Distance.COSINE)
            },
            sparse_vectors_config={
                "keywords": SparseVectorParams()
            }
        )
        
        # Payload ì¸ë±ì‹±
        self.create_payload_indexes()
    
    def create_payload_indexes(self):
        indexes = [
            ("core.created_at", PayloadSchemaType.INTEGER),
            ("core.status", PayloadSchemaType.KEYWORD),
            ("core.priority", PayloadSchemaType.INTEGER),
            ("classification.category", PayloadSchemaType.KEYWORD),
            ("classification.tags", PayloadSchemaType.KEYWORD),
            ("relations.customer_tier", PayloadSchemaType.KEYWORD),
            ("metrics.satisfaction_score", PayloadSchemaType.FLOAT)
        ]
        
        for field_path, schema_type in indexes:
            self.client.create_payload_index(
                collection_name="documents_v2",
                field_name=field_path,
                field_schema=schema_type
            )
```

### Phase 2: ê³ ê¸‰ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ (Week 2)

#### Task 2.1: ë³µí•© í•„í„° ë¹Œë” êµ¬í˜„
**ë‹´ë‹¹**: Backend Developer  
**ì˜ˆìƒ ì‹œê°„**: 20ì‹œê°„

```python
# backend/core/search/filter_builder.py
from qdrant_client.models import Filter, FieldCondition, Range, MatchAny
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta

class AdvancedFilterBuilder:
    """ë³µì¡í•œ ì¡°ê±´ì„ Qdrant Filterë¡œ ë³€í™˜"""
    
    def build_filter(self, conditions: Dict[str, Any]) -> Filter:
        must_conditions = []
        should_conditions = []
        must_not_conditions = []
        
        # ì‹œê°„ ë²”ìœ„ í•„í„°
        if "date_range" in conditions:
            must_conditions.append(self._build_date_filter(conditions["date_range"]))
        
        # ìƒíƒœ í•„í„°
        if "status" in conditions:
            must_conditions.append(
                FieldCondition(
                    key="core.status",
                    match=MatchAny(any=conditions["status"])
                )
            )
        
        # ìš°ì„ ìˆœìœ„ í•„í„°
        if "priority" in conditions:
            must_conditions.append(
                FieldCondition(
                    key="core.priority",
                    range=Range(gte=conditions["priority"]["min"])
                )
            )
        
        # ì¹´í…Œê³ ë¦¬ í•„í„°
        if "categories" in conditions:
            must_conditions.append(
                FieldCondition(
                    key="classification.category",
                    match=MatchAny(any=conditions["categories"])
                )
            )
        
        # íƒœê·¸ í•„í„° (ANY match)
        if "tags" in conditions:
            should_conditions.append(
                FieldCondition(
                    key="classification.tags",
                    match=MatchAny(any=conditions["tags"])
                )
            )
        
        # ê°ì • ì ìˆ˜ í•„í„°
        if "sentiment_range" in conditions:
            must_conditions.append(
                FieldCondition(
                    key="metrics.sentiment_score",
                    range=Range(
                        gte=conditions["sentiment_range"]["min"],
                        lte=conditions["sentiment_range"]["max"]
                    )
                )
            )
        
        return Filter(
            must=must_conditions if must_conditions else None,
            should=should_conditions if should_conditions else None,
            must_not=must_not_conditions if must_not_conditions else None
        )
    
    def _build_date_filter(self, date_range: Dict) -> FieldCondition:
        """ë‚ ì§œ ë²”ìœ„ í•„í„° ìƒì„±"""
        if "relative" in date_range:
            # "ì§€ë‚œ 7ì¼" ê°™ì€ ìƒëŒ€ì  ë‚ ì§œ
            days = date_range["relative"]["days"]
            start_timestamp = int((datetime.now() - timedelta(days=days)).timestamp())
            return FieldCondition(
                key="core.created_at",
                range=Range(gte=start_timestamp)
            )
        else:
            # ì ˆëŒ€ ë‚ ì§œ ë²”ìœ„
            return FieldCondition(
                key="core.created_at",
                range=Range(
                    gte=int(date_range["start"].timestamp()),
                    lte=int(date_range["end"].timestamp())
                )
            )
```

#### Task 2.2: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„
**ë‹´ë‹¹**: Backend Developer  
**ì˜ˆìƒ ì‹œê°„**: 24ì‹œê°„

```python
# backend/core/search/hybrid_search_engine.py
from typing import List, Dict, Any, Optional
import numpy as np

class HybridSearchEngine:
    """ì˜ë¯¸ ê²€ìƒ‰ + í•„í„°ë§ + í‚¤ì›Œë“œ ë§¤ì¹­ í†µí•©"""
    
    async def search(
        self,
        query: str,
        filters: Dict[str, Any],
        search_config: Dict[str, Any]
    ) -> List[Dict]:
        
        # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_vectors = {
            "content": await self.get_embedding(query, model="content"),
            "title": await self.get_embedding(query, model="title")
        }
        
        # 2. í‚¤ì›Œë“œ ì¶”ì¶œ (sparse vectorìš©)
        keywords = self.extract_keywords(query)
        sparse_vector = self.create_sparse_vector(keywords)
        
        # 3. í•„í„° êµ¬ì„±
        qdrant_filter = self.filter_builder.build_filter(filters)
        
        # 4. ë©€í‹° ìŠ¤í…Œì´ì§€ ê²€ìƒ‰
        results = []
        
        # Stage 1: Dense vector ê²€ìƒ‰ (ì˜ë¯¸ ê¸°ë°˜)
        semantic_results = await self.client.search(
            collection_name="documents_v2",
            query_vector=("content", query_vectors["content"]),
            query_filter=qdrant_filter,
            limit=search_config.get("semantic_limit", 50),
            with_payload=True
        )
        
        # Stage 2: Sparse vector ê²€ìƒ‰ (í‚¤ì›Œë“œ ê¸°ë°˜)
        if sparse_vector:
            keyword_results = await self.client.search(
                collection_name="documents_v2",
                query_vector=("keywords", sparse_vector),
                query_filter=qdrant_filter,
                limit=search_config.get("keyword_limit", 30),
                with_payload=True
            )
        
        # Stage 3: ê²°ê³¼ ìœµí•© ë° ë¦¬ë­í‚¹
        fused_results = self.fuse_results(
            semantic_results,
            keyword_results,
            weights=search_config.get("fusion_weights", {"semantic": 0.7, "keyword": 0.3})
        )
        
        # Stage 4: í›„ì²˜ë¦¬ ë° ë©”íƒ€ë°ì´í„° ì¦ê°•
        enriched_results = self.enrich_results(fused_results)
        
        return enriched_results
    
    def fuse_results(
        self,
        semantic_results: List,
        keyword_results: List,
        weights: Dict[str, float]
    ) -> List[Dict]:
        """Reciprocal Rank Fusion"""
        scores = {}
        
        # Semantic ì ìˆ˜ ê³„ì‚°
        for i, result in enumerate(semantic_results):
            doc_id = result.id
            scores[doc_id] = weights["semantic"] / (i + 1)
        
        # Keyword ì ìˆ˜ ì¶”ê°€
        for i, result in enumerate(keyword_results):
            doc_id = result.id
            if doc_id in scores:
                scores[doc_id] += weights["keyword"] / (i + 1)
            else:
                scores[doc_id] = weights["keyword"] / (i + 1)
        
        # ì ìˆ˜ë³„ ì •ë ¬
        sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        
        return [self.get_document(doc_id) for doc_id, _ in sorted_docs]
```

### Phase 3: ìì—°ì–´ ì¿¼ë¦¬ ì²˜ë¦¬ (Week 3)

#### Task 3.1: NLU íŒŒì„œ êµ¬í˜„
**ë‹´ë‹¹**: Backend Developer + AI Engineer  
**ì˜ˆìƒ ì‹œê°„**: 24ì‹œê°„

```python
# backend/core/nlp/query_parser.py
from typing import Dict, Any, List
import re
from datetime import datetime, timedelta

class NaturalLanguageQueryParser:
    """ìì—°ì–´ë¥¼ êµ¬ì¡°í™”ëœ ê²€ìƒ‰ ì¡°ê±´ìœ¼ë¡œ ë³€í™˜"""
    
    def __init__(self):
        self.time_patterns = {
            r"ì§€ë‚œ\s*(\d+)ì¼": lambda x: {"relative": {"days": int(x.group(1))}},
            r"ì˜¤ëŠ˜": lambda x: {"relative": {"days": 0}},
            r"ì–´ì œ": lambda x: {"relative": {"days": 1}},
            r"ì´ë²ˆ\s*ì£¼": lambda x: {"relative": {"days": 7}},
            r"ì§€ë‚œ\s*ì£¼": lambda x: {"relative": {"days": 7, "offset": 7}},
            r"ì´ë²ˆ\s*ë‹¬": lambda x: {"relative": {"days": 30}},
        }
        
        self.priority_patterns = {
            r"ê¸´ê¸‰": {"min": 4, "max": 4},
            r"ë†’ì€\s*ìš°ì„ ìˆœìœ„": {"min": 3, "max": 4},
            r"ì¤‘ìš”": {"min": 3, "max": 4},
            r"ë‚®ì€\s*ìš°ì„ ìˆœìœ„": {"min": 1, "max": 2}
        }
        
        self.status_keywords = {
            "ë¯¸í•´ê²°": ["open", "pending"],
            "í•´ê²°ëœ": ["resolved", "closed"],
            "ì§„í–‰ì¤‘": ["pending"],
            "ëŒ€ê¸°ì¤‘": ["pending"],
            "ì™„ë£Œ": ["closed"]
        }
        
        self.category_mappings = {
            "ê²°ì œ": "billing",
            "í™˜ë¶ˆ": "refund",
            "ê¸°ìˆ ": "technical",
            "ê³„ì •": "account",
            "ë°°ì†¡": "shipping"
        }
    
    async def parse(self, query: str) -> Dict[str, Any]:
        """ìì—°ì–´ ì¿¼ë¦¬ë¥¼ êµ¬ì¡°í™”ëœ ì¡°ê±´ìœ¼ë¡œ íŒŒì‹±"""
        
        conditions = {
            "original_query": query,
            "filters": {},
            "search_text": query  # í•„í„° í‚¤ì›Œë“œ ì œê±° í›„ ë‚¨ì€ í…ìŠ¤íŠ¸
        }
        
        # 1. ì‹œê°„ ì¡°ê±´ ì¶”ì¶œ
        for pattern, extractor in self.time_patterns.items():
            match = re.search(pattern, query)
            if match:
                conditions["filters"]["date_range"] = extractor(match)
                query = re.sub(pattern, "", query)
        
        # 2. ìš°ì„ ìˆœìœ„ ì¶”ì¶œ
        for pattern, priority_range in self.priority_patterns.items():
            if re.search(pattern, query):
                conditions["filters"]["priority"] = priority_range
                query = re.sub(pattern, "", query)
        
        # 3. ìƒíƒœ ì¶”ì¶œ
        for keyword, statuses in self.status_keywords.items():
            if keyword in query:
                conditions["filters"]["status"] = statuses
                query = query.replace(keyword, "")
        
        # 4. ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        categories = []
        for ko_term, en_term in self.category_mappings.items():
            if ko_term in query:
                categories.append(en_term)
                query = query.replace(ko_term, "")
        
        if categories:
            conditions["filters"]["categories"] = categories
        
        # 5. ê³ ê° ë“±ê¸‰ ì¶”ì¶œ
        if "VIP" in query or "vip" in query:
            conditions["filters"]["customer_tier"] = ["vip"]
            query = re.sub(r"[Vv][Ii][Pp]", "", query)
        
        # 6. ê°ì • ë¶„ì„ ì¡°ê±´
        if "ë¶ˆë§Œ" in query or "í™”ë‚œ" in query:
            conditions["filters"]["sentiment_range"] = {"min": -1.0, "max": -0.3}
        elif "ë§Œì¡±" in query or "ê¸ì •" in query:
            conditions["filters"]["sentiment_range"] = {"min": 0.3, "max": 1.0}
        
        # 7. LLM ê¸°ë°˜ ì˜ë„ ë¶„ë¥˜ (ì„ íƒì‚¬í•­)
        if self.use_llm_parsing:
            llm_conditions = await self.llm_parse(query)
            conditions["filters"].update(llm_conditions)
        
        # 8. ì •ì œëœ ê²€ìƒ‰ í…ìŠ¤íŠ¸
        conditions["search_text"] = query.strip()
        
        return conditions
    
    async def llm_parse(self, query: str) -> Dict[str, Any]:
        """LLMì„ ì‚¬ìš©í•œ ê³ ê¸‰ ì¿¼ë¦¬ íŒŒì‹±"""
        prompt = f"""
        ë‹¤ìŒ ìì—°ì–´ ì¿¼ë¦¬ë¥¼ êµ¬ì¡°í™”ëœ ê²€ìƒ‰ ì¡°ê±´ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”:
        ì¿¼ë¦¬: "{query}"
        
        ì¶”ì¶œí•  ì •ë³´:
        - ì‹œê°„ ë²”ìœ„ (date_range)
        - ì¹´í…Œê³ ë¦¬ (categories)
        - íƒœê·¸ (tags)
        - ìš°ì„ ìˆœìœ„ (priority)
        - ìƒíƒœ (status)
        - ê°ì • (sentiment)
        - ì˜ë„ (intent)
        
        JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜:
        """
        
        # LLM í˜¸ì¶œ ë° íŒŒì‹±
        response = await self.llm_client.generate(prompt)
        return self.parse_llm_response(response)
```

#### Task 3.2: ì¿¼ë¦¬ ìµœì í™”ê¸°
**ë‹´ë‹¹**: Backend Developer  
**ì˜ˆìƒ ì‹œê°„**: 16ì‹œê°„

```python
# backend/core/search/query_optimizer.py
class QueryOptimizer:
    """ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ë° ìºì‹±"""
    
    def optimize_query(self, parsed_query: Dict) -> Dict:
        """ì¿¼ë¦¬ ìµœì í™”"""
        optimized = parsed_query.copy()
        
        # 1. í•„í„° ìˆœì„œ ìµœì í™” (ì„ íƒë„ê°€ ë†’ì€ í•„í„° ìš°ì„ )
        if "filters" in optimized:
            optimized["filters"] = self.reorder_filters(optimized["filters"])
        
        # 2. ê²€ìƒ‰ ì „ëµ ê²°ì •
        optimized["search_strategy"] = self.determine_strategy(optimized)
        
        # 3. ìºì‹œ í‚¤ ìƒì„±
        optimized["cache_key"] = self.generate_cache_key(optimized)
        
        # 4. ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ìµœì í™”
        optimized["search_params"] = {
            "semantic_limit": self.calculate_limit(optimized, "semantic"),
            "keyword_limit": self.calculate_limit(optimized, "keyword"),
            "fusion_weights": self.calculate_fusion_weights(optimized)
        }
        
        return optimized
    
    def determine_strategy(self, query: Dict) -> str:
        """ìµœì  ê²€ìƒ‰ ì „ëµ ê²°ì •"""
        filters = query.get("filters", {})
        
        # í•„í„°ê°€ ë§ìœ¼ë©´ í•„í„° ìš°ì„  ì „ëµ
        if len(filters) > 3:
            return "filter_first"
        
        # ì‹œê°„ ë²”ìœ„ê°€ ì¢ìœ¼ë©´ ì‹œê°„ ìš°ì„  ì „ëµ
        if "date_range" in filters:
            if filters["date_range"].get("relative", {}).get("days", 30) < 7:
                return "recent_first"
        
        # ê¸°ë³¸: í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ
        return "hybrid"
```

### Phase 4: ì„±ëŠ¥ ìµœì í™” (Week 4)

#### Task 4.1: ì¸ë±ì‹± ìµœì í™”
**ë‹´ë‹¹**: Backend Developer  
**ì˜ˆìƒ ì‹œê°„**: 16ì‹œê°„

```python
# backend/core/database/index_optimizer.py
class IndexOptimizer:
    """Qdrant ì¸ë±ìŠ¤ ìµœì í™”"""
    
    async def optimize_collection(self):
        # 1. HNSW íŒŒë¼ë¯¸í„° íŠœë‹
        await self.client.update_collection(
            collection_name="documents_v2",
            optimizer_config={
                "indexing_threshold": 20000,
                "flush_interval_sec": 5,
                "max_optimization_threads": 4
            },
            hnsw_config={
                "m": 16,  # ì—°ê²° ìˆ˜
                "ef_construct": 200,  # êµ¬ì¶• ì‹œ íƒìƒ‰ ê¹Šì´
                "full_scan_threshold": 10000
            }
        )
        
        # 2. Quantization ì„¤ì •
        await self.client.update_collection(
            collection_name="documents_v2",
            quantization_config={
                "scalar": {
                    "type": "int8",
                    "quantile": 0.99,
                    "always_ram": True
                }
            }
        )
```

#### Task 4.2: ìºì‹± ë ˆì´ì–´
**ë‹´ë‹¹**: Backend Developer  
**ì˜ˆìƒ ì‹œê°„**: 12ì‹œê°„

```python
# backend/core/cache/search_cache.py
from typing import Optional, Dict, Any
import hashlib
import json
import redis.asyncio as redis

class SearchCache:
    """ê²€ìƒ‰ ê²°ê³¼ ìºì‹±"""
    
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.ttl = 3600  # 1ì‹œê°„
    
    async def get(self, query: Dict) -> Optional[Dict]:
        """ìºì‹œëœ ê²°ê³¼ ì¡°íšŒ"""
        cache_key = self.generate_key(query)
        cached = await self.redis.get(cache_key)
        
        if cached:
            return json.loads(cached)
        return None
    
    async def set(self, query: Dict, results: Dict):
        """ê²°ê³¼ ìºì‹±"""
        cache_key = self.generate_key(query)
        await self.redis.setex(
            cache_key,
            self.ttl,
            json.dumps(results)
        )
    
    def generate_key(self, query: Dict) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        # ì¿¼ë¦¬ë¥¼ ì •ê·œí™”í•˜ì—¬ ì¼ê´€ëœ í‚¤ ìƒì„±
        normalized = {
            "text": query.get("search_text", ""),
            "filters": sorted(json.dumps(query.get("filters", {})))
        }
        
        key_string = json.dumps(normalized, sort_keys=True)
        return f"search:v2:{hashlib.md5(key_string.encode()).hexdigest()}"
```

### Phase 5: í†µí•© ë° í…ŒìŠ¤íŠ¸ (Week 5)

#### Task 5.1: API ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„
**ë‹´ë‹¹**: Backend Developer  
**ì˜ˆìƒ ì‹œê°„**: 16ì‹œê°„

```python
# backend/api/routes/advanced_search.py
from fastapi import APIRouter, Depends, Query
from typing import Optional, Dict, Any

router = APIRouter(prefix="/api/v2/search")

@router.post("/natural")
async def natural_language_search(
    query: str,
    tenant_id: str,
    options: Optional[Dict] = None
):
    """ìì—°ì–´ ê²€ìƒ‰ ì—”ë“œí¬ì¸íŠ¸"""
    
    # 1. ì¿¼ë¦¬ íŒŒì‹±
    parsed = await query_parser.parse(query)
    
    # 2. ì¿¼ë¦¬ ìµœì í™”
    optimized = query_optimizer.optimize_query(parsed)
    
    # 3. ìºì‹œ í™•ì¸
    cached = await search_cache.get(optimized)
    if cached:
        return cached
    
    # 4. ê²€ìƒ‰ ì‹¤í–‰
    results = await hybrid_search_engine.search(
        query=optimized["search_text"],
        filters=optimized["filters"],
        search_config=optimized["search_params"]
    )
    
    # 5. ê²°ê³¼ ìºì‹±
    await search_cache.set(optimized, results)
    
    # 6. ì‘ë‹µ í¬ë§·íŒ…
    return {
        "query": query,
        "parsed_conditions": parsed,
        "results": results,
        "total": len(results),
        "search_strategy": optimized["search_strategy"]
    }

@router.post("/advanced")
async def advanced_search(
    conditions: Dict[str, Any],
    tenant_id: str
):
    """êµ¬ì¡°í™”ëœ ê³ ê¸‰ ê²€ìƒ‰"""
    
    # í•„í„° ë¹Œë”ë¡œ ì§ì ‘ ê²€ìƒ‰
    filter_obj = filter_builder.build_filter(conditions)
    
    results = await client.search(
        collection_name="documents_v2",
        query_filter=filter_obj,
        limit=conditions.get("limit", 100)
    )
    
    return {
        "conditions": conditions,
        "results": results,
        "total": len(results)
    }
```

#### Task 5.2: í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸
**ë‹´ë‹¹**: QA Engineer  
**ì˜ˆìƒ ì‹œê°„**: 20ì‹œê°„

```python
# tests/test_advanced_search.py
import pytest
from datetime import datetime, timedelta

class TestAdvancedSearch:
    
    @pytest.mark.asyncio
    async def test_natural_language_parsing(self):
        """ìì—°ì–´ íŒŒì‹± í…ŒìŠ¤íŠ¸"""
        test_cases = [
            {
                "query": "ì§€ë‚œ 7ì¼ê°„ ê¸´ê¸‰í•œ ê²°ì œ ë¬¸ì œ",
                "expected_filters": {
                    "date_range": {"relative": {"days": 7}},
                    "priority": {"min": 4, "max": 4},
                    "categories": ["billing"]
                }
            },
            {
                "query": "VIP ê³ ê°ì˜ ë¯¸í•´ê²° í™˜ë¶ˆ ìš”ì²­",
                "expected_filters": {
                    "customer_tier": ["vip"],
                    "status": ["open", "pending"],
                    "categories": ["refund"]
                }
            }
        ]
        
        for case in test_cases:
            result = await query_parser.parse(case["query"])
            assert result["filters"] == case["expected_filters"]
    
    @pytest.mark.asyncio
    async def test_complex_filter_building(self):
        """ë³µí•© í•„í„° ìƒì„± í…ŒìŠ¤íŠ¸"""
        conditions = {
            "date_range": {"relative": {"days": 30}},
            "status": ["open", "pending"],
            "priority": {"min": 3},
            "categories": ["billing", "refund"],
            "tags": ["urgent", "vip"],
            "sentiment_range": {"min": -1.0, "max": -0.5}
        }
        
        filter_obj = filter_builder.build_filter(conditions)
        
        assert len(filter_obj.must) >= 4
        assert len(filter_obj.should) >= 1
    
    @pytest.mark.asyncio
    async def test_search_performance(self):
        """ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        import time
        
        query = "ìµœê·¼ ì¼ì£¼ì¼ ë‚´ ê¸´ê¸‰í•œ ê¸°ìˆ  ë¬¸ì œ"
        
        start = time.time()
        results = await natural_language_search(
            query=query,
            tenant_id="test_tenant"
        )
        elapsed = time.time() - start
        
        assert elapsed < 1.0  # 1ì´ˆ ì´ë‚´ ì‘ë‹µ
        assert len(results["results"]) > 0
```

## ğŸ“Š ê²€ì¦ ë©”íŠ¸ë¦­

### ì„±ëŠ¥ ëª©í‘œ
- **ê²€ìƒ‰ ì§€ì—°ì‹œê°„**: < 500ms (P95)
- **ì¿¼ë¦¬ íŒŒì‹± ì‹œê°„**: < 50ms
- **ìºì‹œ íˆíŠ¸ìœ¨**: > 60%
- **ì¸ë±ìŠ¤ í¬ê¸°**: < ì›ë³¸ ë°ì´í„°ì˜ 150%

### ì •í™•ë„ ëª©í‘œ
- **ìì—°ì–´ íŒŒì‹± ì •í™•ë„**: > 90%
- **ê²€ìƒ‰ ê´€ë ¨ì„± (nDCG)**: > 0.85
- **í•„í„° ì •í™•ë„**: 100%

## ğŸš€ ë°°í¬ ì „ëµ

### Stage 1: ê°œë°œ í™˜ê²½ (Week 1-4)
- ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± ë° í…ŒìŠ¤íŠ¸
- ê¸°ì¡´ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜

### Stage 2: ìŠ¤í…Œì´ì§• í™˜ê²½ (Week 5)
- A/B í…ŒìŠ¤íŠ¸ ì„¤ì •
- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹

### Stage 3: í”„ë¡œë•ì…˜ ë°°í¬ (Week 6)
- ì ì§„ì  ë¡¤ì•„ì›ƒ (10% â†’ 50% â†’ 100%)
- ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

## ğŸ¯ ì„±ê³µ ê¸°ì¤€

### ê¸°ìˆ ì  ì„±ê³µ ì§€í‘œ
- âœ… ë³µì¡í•œ ì¡°ê±´ ì²˜ë¦¬ ê°€ëŠ¥
- âœ… ìì—°ì–´ ì¿¼ë¦¬ ì§€ì›
- âœ… 500ms ì´ë‚´ ì‘ë‹µ
- âœ… 90% ì´ìƒ íŒŒì‹± ì •í™•ë„

### ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ê³µ ì§€í‘œ
- ğŸ“ˆ ê²€ìƒ‰ ë§Œì¡±ë„ 30% í–¥ìƒ
- ğŸ“ˆ í‰ê·  ê²€ìƒ‰ ì‹œê°„ 50% ë‹¨ì¶•
- ğŸ“ˆ ì§€ì› í‹°ì¼“ í•´ê²° ì‹œê°„ 20% ê°ì†Œ

## ğŸ“š ì°¸ê³  ìë£Œ

### Qdrant ë¬¸ì„œ
- [Filtering Documentation](https://qdrant.tech/documentation/concepts/filtering/)
- [Payload Indexing](https://qdrant.tech/documentation/concepts/indexing/)
- [Hybrid Search](https://qdrant.tech/documentation/tutorials/hybrid-search/)

### êµ¬í˜„ ì˜ˆì œ
- [Natural Language Search](https://github.com/qdrant/examples/nlp-search)
- [Multi-Stage Retrieval](https://github.com/qdrant/examples/multi-stage)

---

*Generated with Qdrant Optimization Expertise + Sequential Analysis*  
*ìµœì¢… ì—…ë°ì´íŠ¸: 2025ë…„ 1ì›”*